# ğŸ  Dimensionality Reduction and Clustering in California Housing Dataset

## ğŸ¯ Objective
Explore the data structure of the **California Housing Dataset** through **dimensionality reduction** and **unsupervised clustering**, evaluating whether it's possible to identify patterns similar to the categorical variable `ocean_proximity` without using it directly in training.

---

## ğŸ“‚ Dataset
- **Source**: California Housing Dataset.
- **Rows**: 20,640
- **Columns**: 9 (numerical + `ocean_proximity` categorical)
- **Target categorical variable (evaluation only)**: `ocean_proximity`.

---

## ğŸ›  Methodology

### 1. Preprocessing
- Encoding of categorical variable using `OneHotEncoder`.
- Scaling of numerical variables using `StandardScaler`.
- Unified pipeline with `ColumnTransformer`.

---

### 2. PCA (Principal Component Analysis)
- Reduction to **2 components**.
- **Cumulative explained variance**: ~62%.
- Visualization colored by `ocean_proximity` â†’ high overlap.
- Clustering with **K-Means (K=5)** â†’ **ARI = 0.125** â†’ low similarity with real categories.

---

### 3. t-SNE (Non-Linear Dimensionality Reduction)
- Reduction to 2 dimensions preserving non-linear relationships.
- Initial parameterization (`perplexity=30`, `learning_rate=200`) showed greater visual separation.
- Hyperparameter optimization:
  - Best combination: **perplexity=30, learning_rate=500**.
  - **ARI with K-Means on optimized t-SNE = 0.419** â†’ significant improvement over PCA.

---

### 4. Clustering (K-Means)
- **Number of clusters (K)**: equal to the number of real categories (5).
- Evaluation with **Adjusted Rand Index (ARI)** using `ocean_proximity` as reference.
- Contingency table â†’ several clusters clearly represent specific categories (e.g., `INLAND`, `NEAR BAY`).

---

## ğŸ“Š Results

| Technique                    | ARI   | Observations |
|------------------------------|-------|---------------|
| PCA + K-Means                | 0.125 | Much overlap, insufficient linear structure. |
| Initial t-SNE + K-Means      | 0.341 | Notable improvement, more visually defined groups. |
| Optimized t-SNE + K-Means    | 0.419 | Better separation, several clusters aligned with real categories. |

---

## ğŸ“ˆ Comparative Visualization
- **Left**: Optimized t-SNE colored by real categories.  
- **Right**: Optimized t-SNE colored by K-Means clusters.  
*(Insert here the images generated in the notebook)*

---

## ğŸ§  Conclusions
- PCA is useful for quick visualization and reduction, but limited to linear patterns.
- t-SNE captures non-linear relationships, achieving better separation in this dataset.
- Even without using `ocean_proximity` for training, clustering detected part of its structure.
- ARI = 0.419 â†’ moderate correlation between clusters and real categories.

---

## ğŸš€ Next Steps
- Test **UMAP** as a faster and more scalable alternative to t-SNE.
- Enrich features with geographic transformations to further improve clustering.
- Use more flexible clustering methods (Gaussian Mixtures, optimized DBSCAN).

---

## âš™ï¸ Execution Requirements
Install necessary dependencies:
```bash
pip install -r requirements.txt
```

---

## ğŸ“¦ Main Dependencies
```
numpy
pandas
scikit-learn
matplotlib
seaborn
```

---

## ğŸ”§ Usage

### Run the complete notebook:
```bash
jupyter notebook dimensionality_reduction_clustering.ipynb
```

### Run the main script:
```python
python main.py
```

---

## ğŸ“ Project Structure
```
california-housing-clustering/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ housing.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ dimensionality_reduction_clustering.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ dimensionality_reduction.py
â”‚   â””â”€â”€ clustering.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ pca_visualization.png
â”‚   â”œâ”€â”€ tsne_visualization.png
â”‚   â””â”€â”€ comparison_plot.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Key Metrics

### PCA
- **Explained Variance (2 components)**: 62%
- **ARI with K-Means**: 0.125

### t-SNE (Optimized)
- **Perplexity**: 30
- **Learning Rate**: 500
- **ARI with K-Means**: 0.419

---

## ğŸ“ Educational Value

This project demonstrates:
- Effective use of dimensionality reduction techniques
- Comparison between linear (PCA) and non-linear (t-SNE) methods
- Unsupervised clustering evaluation with ARI
- Hyperparameter optimization for t-SNE
- Data visualization best practices

---

## ğŸ‘¨â€ğŸ’» Author

Developed as part of Machine Learning coursework exploring unsupervised learning techniques.

---

## ğŸ“„ License

This project is open source and available under the MIT License.

---

## ğŸ”— References

- [Scikit-learn Documentation](https://scikit-learn.org/)
- [t-SNE: A Tutorial](https://distill.pub/2016/misread-tsne/)
- [California Housing Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html)
